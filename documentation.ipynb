{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db948235",
   "metadata": {},
   "source": [
    "# Problem Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48a5f8",
   "metadata": {},
   "source": [
    "The objective of this project is to build an image classifier capable of distinguishing between cats and dogs using CNN.\n",
    "\n",
    "Traditional ML models like Linear Regression or Logistic Regression are not suitable for image data because images contain complex features. Convolutional Neural Network (CNN) was selected since CNNs are designed to extract features like edges, textures, and shapes from images by processing the images in layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83005d7e",
   "metadata": {},
   "source": [
    "# Data Collection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d97a6e",
   "metadata": {},
   "source": [
    "The dataset(https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition/data) consists of labeled images stored in 'dataset' folder. Real-world image data containing pictures of cats and dogs was used. \n",
    "\n",
    "Binary Classification was done to classify the images using 'ImageFolder' class of PyTorch as follows:\n",
    "\n",
    "Cat -> 0\n",
    "Dog -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948670a",
   "metadata": {},
   "source": [
    "# EDA & Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaed1d",
   "metadata": {},
   "source": [
    "All the images are resized to 128 * 128 pixels.\n",
    "\n",
    "To improve model generation, random horizontal flip was applied during training.\n",
    "\n",
    "The images were converted into Tensors (Multi-dimensional arrays) and pixel values were scaled from 0 to 255 to 0 to 1.\n",
    "\n",
    "Invalid or unreadable images were automatically ignored during dataset loading.\n",
    "\n",
    "80% of images for dogs were used for training whereas the rest of the images were used for testing. The same train-test split was made for images of cats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece58e9e",
   "metadata": {},
   "source": [
    "# Model Building:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7ef2d",
   "metadata": {},
   "source": [
    "1.We Input an Image with 3 Channels (Red, Green, Blue) of size 128 * 128 pixels.\n",
    "\n",
    "2.First Convolution takes the image with 3 Channels and applies 32 filters (3*3 matrices) to obtain feature maps. ReLU is used to introduce non-linearity so that the network can learn curves and complex shapes.\n",
    "\n",
    "3.Second Convolution takes the 32 filters from first convolution as input and applies 64 filters (3*3 matrices) to obtain feature maps. ReLU is used to introduce non-linearity so that the network can learn curves and complex shapes.\n",
    "\n",
    "4.Third Convolution takes the 64 filters from second convolution as input and applies 128 filters (3*3 matrices) to obtain feature maps. ReLU is used to introduce non-linearity so that the network can learn curves and complex shapes. This helps identify more patterns.\n",
    "\n",
    "5.After each Convolution,pooling is done to the feature maps to make them smaller and keep only the most important informations. we use Max Pooling (maximum value from each 2Ã—2 region) which halves the dimension each time it is applied.\n",
    "\n",
    "6.After 3 pooling operations, 128 feature maps, each of size 16 * 16, are flattened into one long vector of 32,768 numbers. This vector is compressed into 256 numbers.\n",
    "\n",
    "7.Again, in FC2 (Second Fully Connected Layer), the 256 values are compressed into 1 single number which is the model's prediction.\n",
    "\n",
    "8.During training, dropout is applied which randomly turns off 50% of neurons, forcing the network to not rely on any single neuron and learn more features.\n",
    "\n",
    "9.Finally, the model predicts using probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb23ad",
   "metadata": {},
   "source": [
    "# Running the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061528f7",
   "metadata": {},
   "source": [
    "#### Python Version 3.11.9 is used since latest versions do not support PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c778a024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1\n",
      "Loss: 0.6641\n",
      "Accuracy: 69.04%\n",
      "\n",
      "Epoch 2\n",
      "Loss: 0.5575\n",
      "Accuracy: 75.26%\n",
      "\n",
      "Epoch 3\n",
      "Loss: 0.4882\n",
      "Accuracy: 77.76%\n",
      "\n",
      "Epoch 4\n",
      "Loss: 0.4409\n",
      "Accuracy: 79.54%\n",
      "\n",
      "Epoch 5\n",
      "Loss: 0.4035\n",
      "Accuracy: 79.98%\n",
      "\n",
      "Epoch 6\n",
      "Loss: 0.3705\n",
      "Accuracy: 83.02%\n",
      "\n",
      "Epoch 7\n",
      "Loss: 0.3411\n",
      "Accuracy: 83.08%\n",
      "\n",
      "Epoch 8\n",
      "Loss: 0.3143\n",
      "Accuracy: 83.16%\n",
      "\n",
      "Epoch 9\n",
      "Loss: 0.2819\n",
      "Accuracy: 83.24%\n",
      "\n",
      "Epoch 10\n",
      "Loss: 0.2587\n",
      "Accuracy: 83.70%\n",
      "\n",
      "Epoch 11\n",
      "Loss: 0.2350\n",
      "Accuracy: 83.92%\n",
      "\n",
      "Epoch 12\n",
      "Loss: 0.2120\n",
      "Accuracy: 85.08%\n",
      "\n",
      "cnn.pth created!\n"
     ]
    }
   ],
   "source": [
    "#For training:\n",
    "!py -3.11 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38932362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a  dog.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ImageClassifier (Dog or Cat)\\test.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"cnn.pth\"))\n"
     ]
    }
   ],
   "source": [
    "#For prediction of image (You can edit the image name in line 17 of test.py to try on other images after adding the images in the folder with model.py, test.py and cnn.pth):\n",
    "!py -3.11 test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca48094",
   "metadata": {},
   "source": [
    "# Model Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e301c65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['cats', 'dogs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Detoxified\\AppData\\Local\\Temp\\ipykernel_15212\\2030721718.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"cnn.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2145  355]\n",
      " [ 306 2194]]\n",
      " \n",
      "cats:\n",
      " Precision : 0.8752\n",
      " Recall    : 0.8580\n",
      " F1-Score  : 0.8665\n",
      "\n",
      "dogs:\n",
      " Precision : 0.8607\n",
      " Recall    : 0.8776\n",
      " F1-Score  : 0.8691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from model import CNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    \"dataset/test\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "classes = test_dataset.classes\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(\"cnn.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        \n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds).flatten()\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\" \")\n",
    "for i, class_name in enumerate(classes):\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=i)\n",
    "    recall = recall_score(all_labels, all_preds, pos_label=i)\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=i)\n",
    "\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\" Precision : {precision:.4f}\")\n",
    "    print(f\" Recall    : {recall:.4f}\")\n",
    "    print(f\" F1-Score  : {f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395410a",
   "metadata": {},
   "source": [
    "# Interpretation & Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09654be",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "An accuracy of approximately 85.08% was seen during training with a loss of 0.2120. Dropout helped reduce overfitting and random horizontal flips during training improved generalization.\n",
    "\n",
    "#### Insights:\n",
    "The model identified different complex features of the image such as facial structures and shape patterns instead of relying on manually engineered features.\n",
    "\n",
    "#### Conclusion:\n",
    "Convolutional Neural Networks are better for image classification tasks. The trained model could accurately classify unseen images of cats and dogs. This model can be extended to multi-class image classification like done in ResNet-18 where 18 deep layers are used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
